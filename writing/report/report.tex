\documentclass[nobib]{tufte-handout}

\title{Data-Driven Models for Zebrafish Motion}

\author[Lukas Krenz]{Lukas Krenz}

%\geometry{showframe} % display margins for debugging page layout
\usepackage{hyphenat}
\usepackage[
  style=verbose,
  autocite=footnote,
  backend=biber
]{biblatex}
\addbibresource{../bibliography.bib}

\usepackage{caption}
\usepackage{xpatch}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{mathtools} % for \mathclap
\usepackage{varioref}
\usepackage{siunitx}
\usepackage{hyperref}
\usepackage[noabbrev]{cleveref}
\newcommand{\creflastconjunction}{, and\nobreakspace} % use Oxford comma
\usepackage{todonotes}
\usepackage{phaistos}
\usepackage{multimedia}
\usepackage{tikz}
\usetikzlibrary{arrows, positioning, shapes.geometric}
\usetikzlibrary{calc}

\graphicspath{{../../figures/}}

% \usepackage{graphicx} % allow embedded images
%   \setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
%   \graphicspath{{graphics/}} % set of paths to search for images
% \usepackage{booktabs} % book-quality tables
% \usepackage{units}    % non-stacked fractions and better unit spacing
% \usepackage{multicol} % multiple column layout facilities
% \usepackage{fancyvrb} % extended verbatim environments
%   \fvset{fontsize=\normalsize}% default font size for fancy-verbatim environments

% Standardize command font styles and environments
\newcommand{\doccmd}[1]{\texttt{\textbackslash#1}}% command name -- adds backslash automatically
\newcommand{\docopt}[1]{\ensuremath{\langle}\textrm{\textit{#1}}\ensuremath{\rangle}}% optional command argument
\newcommand{\docarg}[1]{\textrm{\textit{#1}}}% (required) command argument
\newcommand{\docenv}[1]{\textsf{#1}}% environment name
\newcommand{\docpkg}[1]{\texttt{#1}}% package name
\newcommand{\doccls}[1]{\texttt{#1}}% document class name
\newcommand{\docclsopt}[1]{\texttt{#1}}% document class option name
\newenvironment{docspec}{\begin{quote}\noindent}{\end{quote}}% command specification environment

\begin{document}

\maketitle% this prints the handout title, author, and date

\begin{abstract}
\noindent
The goal of the project is to compare different data-driven models for the behavior of zebrafish.
The models should be able to predict and simulate the individual motion of an animal reacting to its environment (e.g. another fish, wall).
It can be used to steer a fish in a virtual reality environment, for example.

We start with the evaluation of a simple force based model.
The model consists of two parts, one that models the information processing and one that models the gliding phase.
The gliding is described by a physical model with parameters taken from the data.

We will then enhance the model by using a spatio-temporal (linear) receptive field as the social model that includes information about the perception of the fish.
This modeling strategy is inspired by computational neuroscience and adds a 'memory' to the model.

As a next step, the receptive field will be replaced by a recurrent neural network.
This is a generalization of the previous model and includes possible non-linear reactions and time-dependencies.
\end{abstract}

\section{Introduction}
The goal of the project is to compare different strategies for modelling the behaviour of juvenile zebrafish.
These models should be able to predict and simulate the individual motion of one animal reacting to its environment (e.g. another fish and a wall).

A use-case for this project is steering a virtual fish in a virtual reality environment for animals.
It can be used to perform experiments that investigate causal relationships in animal behaviour.

The movement of zebrafish can be described by discrete models that assume that the fish moves in a piece-wise linear fashion.
After choosing a heading direction, the fish kicks off and moves in an approximately straight line.
We model the heading change for each kick.

We start with an simple model and refine it twice.
Each modification drops assumptions about the behaviour and thus creates a more flexible, data-driven model.
The models developed in the earlier steps serve as baselines for the more complicated ones.
We can thus see whether the assumptions are correct and how important each component of the model is.

We make the following contributions:
\begin{itemize}
\item We adapt a wall model to zebrafish.
\item We develop an adaptive receptive field approach for efficient spatial binning.
\item We evaluate the effect of time-dependet receptive fields.
\item We present a recurrent neural network that can handle bi-modal predictions and captures non-linear effects.

\end{itemize}
\section{Methods: Pre-processing \textit{\&} Models}

\subsection{Discrete Motion}

\subsection{Wall forces}
We begin with the modeling of wall-forces.
We follow the modeling approach of~\autocite{calovi} and describe the influence of a wall on the heading change \(\delta \phi_w\) as
\begin{equation*}
  \delta \phi_w (r_w, \theta_w) = f(r_w)O_w(\theta_w),
\end{equation*}
where $r_w$ corresponds to the distance to the wall and $\theta$ is the relative angle of the fish towars the wall.
We split the wall influence into an exponential decaying force term \(f_w\) and an odd angular-response function \(O_w\).
%todo: multiply by a constant
\begin{align*}
  f(r_w) &= \exp\left( -{(r_w/l_w)}^2 \right), \\
  O(\theta_w) &= \left(a_1 \sin(\theta_w) + a_2 \sin(2  \theta_w)  \right)  \left(1 +  b_1  \cos(\theta_w) + b_2 \cos(2  \theta_w) \right),
\end{align*}
where $l_w, a_1, a_2, b_1 \text{ and } b_2$ are parameters.
Note that we use a different series expansion for the odd angular function \(O_w\)
\footnote{Their proposed form does not work in our case.
  The reason for this could be that they consider both a different species and a round wall.}.
Finally, we consider the closest two walls and sum over the influences
\begin{equation*}
 \delta \phi_w^{\text{total}} \left( \bm{r_w}, \bm{\theta_w} \right) = \sum_{i \in 2 \text{ closest walls}} \delta \phi_w^i (r_w^i, \theta_w^i).
\end{equation*}
The parameters are fit by minimizing the mean-squared error of heading change prediction using gradient-descent.
% todo: we do not actually use gradient descent but something different.

\subsection{Receptive Fields}
We now explain the input features of our social models.
From here on, we restrict our dataset to kicks where the wall influence is neglible.
Practically, we drop all kicks where the value of \(f_w(r_w)\) is larger than $0.1$.

Our social model use a receptive field as their input~\autocite{discreteModes}.
We construct this by rotating the coordinate system such that the fish we are considering is parallel to the x-axis.
This corresponds to a heading of zero degrees.
We then shift the coordinate system such that our fish is at the center.

Or models then try to predict the product of a vector in direction of the heading change and the kick length.
This correspond to the kick trajectory in our new coordinate system.

The position is discretized using adaptive spatial bins that are constructed by\(\cdots\).
\marginnote{This can be generalized to multiple fish by interpreting the features as number of fish per bin and mean relative angle per bin.}
This categorical feature bin is converted using one-hot-encoding to a feature vector.
We additionally use the relative angle of the other fish multiplied with the aforementioned spatial feature.

\subsection{First linear model}
We begin with a basic linear model.
The linear model for a component \(i\) of our target vector \(\hat{\bm{y}}\) can be written as
\begin{equation*}
 \left( Y^i | \bm{\beta}^{i} \right)  \sim \mathcal{N} \left( \bm{X^i} \bm{\beta^i}, \sigma^2 \bm{I_{n \times p}}  \right),
\end{equation*}
where X is the design matrix.
In the following discussion we will drop the superscript for the component and describe the computation for one vector component without loss of generality.

\subsection{Adding a time dependence}
We now add a time-dependence to our model.
To do this, we extract some time window before each kick.
For our experiments we use time steps of \(\SI{0}{\s}, \SI{0.05}{s}, \ldots, \SI{0.25}{\s}\), which goes back roughly to the beginning of the last kick.

We consider two more models here.
For the first one we simply concatenate all features \(\beta_{i, t}\) for all timesteps \(t\).
This results in a very large number of parameters (\(\text{num. timesteps } \times \text{ num. bins}\)).

Alternatively, we can use the same spatial weights for each bin (i.e\ \( \forall t_1, t_2: \beta_{i, t_1} = \beta_{i, t_2}\)).
We then introduce a parameter \(c_t\) for each timestep and then use $c_t \beta_{i,t}$ as our parameters.
For consistency, we normalize all \(c_t\) such that they sum to unity.


\subsection{Adding non-linear effects}
As a final model we present a recurrent-neural network that predicts a full distribution for \(\left( \bm{y} | \bm{\beta} \right)\).
To do this, we write our prediction for
\begin{equation*}
p \left( \bm{y} | \bm{\beta} \right) = \sum_{i}^n \pi_i \mathcal{N} \left( \bm{\mu_i}, \bm{\Sigma_i} \right),
\end{equation*}
where \(\pi_i, \mu_i, \Sigma_i\) are the parameters for a mixture of Gaussians with \(k\) components.
We follow the description of prior work\autocite{mdn} but use multivariate Gaussians with non-diagonal covariance as mixture components.

These parameters are predicted for each target variable seperately by our neural network.
We now write the covariance matrix as
\begin{equation*}
  \bm{\Sigma_i} = \bm{L_i} \bm{L_i^T} =
  \begin{bmatrix}
    a_i^2 & a_ib_i \\
    a_ib_i & b_i^2 + c_i^2
  \end{bmatrix} ,\quad
   \text{with } \bm{L} =
   \begin{bmatrix}
     a_i & 0 \\
     b_i & c_i
   \end{bmatrix},
\end{equation*}
which corresponds to the Cholesky decomposition of \(\bm{\Sigma_i}\).

We now need to apply the following constraints to the parameters to achieve a valid configuration:
\begin{itemize}
\item The mixing coefficients need to be positive and have to sum to one.
  This is achieved by using the \textit{softmax} non-linearity.
\item The diagonals of all \(\bm{L}_i\) need to be larger than zero.
  To do this we apply the \textit{softplus} non-linearity and add a small \(\varepsilon = 10^{-4}\) for numerical stability\footnote{This regularization reduces the condition number of \(\bm{\Sigma_i}\) by increasing the value of its minimal eigenvalue.}. % maybe cite this master's thesis?
  Using the fact that a matrix has an invertible Cholesky decomposition with positive diagonal elements iff.\ it is positive definite and Hermetian,
  the resulting matrix \(\bm{\Sigma_i}\) is a valid non-singular covariance matrix.
\end{itemize}
All other parameters can have arbitrary values.



\section{Implementation \textit{\&} Training Details}
All neural networks were developed with PyTorch.
We use Adam with a learning rate of $0.01$, exponentially decayed (by ...) with a weight decay of 1e-4.


\section{Evaluation}
% TODO: The following is not true!
%\marginnote{We can compare the log-likelihood of our models because they are \textit{nested} models, i.e.\ each model is a non-regularized \textsc{RNN} predicting parameters of mixtures of Gaussians with certain restrictions on the parameters.}
We compare the models by the log-likelihood on the training and testing set.
This comparison captures the whole predictive distribution and not only the mean prediction.
In this way we take uncertainty into account.
Note that it is not straight-forward to use more complicated methods such as log-likelihood ratio or the Akaike information criterion because we cannot estimate the number of degrees of freedom for penalized recurrent neural network models.

\section{Conclusion}

\printbibliography
\end{document}