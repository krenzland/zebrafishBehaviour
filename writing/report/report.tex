\documentclass[nobib]{tufte-handout}

\title{Data-Driven Models for Zebrafish Motion}

\author[Lukas Krenz]{Lukas Krenz}

%\geometry{showframe} % display margins for debugging page layout
\usepackage{hyphenat}
\usepackage[
  style=verbose,
  autocite=footnote,
  backend=biber
]{biblatex}
\addbibresource{../bibliography.bib}

\usepackage{caption}
\usepackage{xpatch}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{mathtools} % for \mathclap
\usepackage{varioref}
\usepackage{siunitx}
\usepackage{hyperref}
\usepackage[noabbrev]{cleveref}
\newcommand{\creflastconjunction}{, and\nobreakspace} % use Oxford comma
\usepackage{todonotes}
\usepackage{phaistos}
\usepackage{multimedia}
\usepackage{tikz}
\usetikzlibrary{arrows, positioning, shapes.geometric}
\usetikzlibrary{calc}

\graphicspath{{../../figures/}}

% \usepackage{graphicx} % allow embedded images
%   \setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
%   \graphicspath{{graphics/}} % set of paths to search for images
% \usepackage{booktabs} % book-quality tables
% \usepackage{units}    % non-stacked fractions and better unit spacing
% \usepackage{multicol} % multiple column layout facilities
% \usepackage{fancyvrb} % extended verbatim environments
%   \fvset{fontsize=\normalsize}% default font size for fancy-verbatim environments

% Standardize command font styles and environments
\newcommand{\doccmd}[1]{\texttt{\textbackslash#1}}% command name -- adds backslash automatically
\newcommand{\docopt}[1]{\ensuremath{\langle}\textrm{\textit{#1}}\ensuremath{\rangle}}% optional command argument
\newcommand{\docarg}[1]{\textrm{\textit{#1}}}% (required) command argument
\newcommand{\docenv}[1]{\textsf{#1}}% environment name
\newcommand{\docpkg}[1]{\texttt{#1}}% package name
\newcommand{\doccls}[1]{\texttt{#1}}% document class name
\newcommand{\docclsopt}[1]{\texttt{#1}}% document class option name
\newenvironment{docspec}{\begin{quote}\noindent}{\end{quote}}% command specification environment

\begin{document}

\maketitle% this prints the handout title, author, and date

\begin{abstract}
\noindent
The goal of the project is to compare different data-driven models for the behavior of zebrafish.
The models should be able to predict and simulate the individual motion of an animal reacting to its environment (e.g. another fish, wall).
It can be used to steer a fish in a virtual reality environment, for example.

We start with the evaluation of a simple force based model.
The model consists of two parts, one that models the information processing and one that models the gliding phase.
The gliding is described by a physical model with parameters taken from the data.

We will then enhance the model by using a spatio-temporal (linear) receptive field as the social model that includes information about the perception of the fish.
This modeling strategy is inspired by computational neuroscience and adds a 'memory' to the model.

As a next step, the receptive field will be replaced by a recurrent neural network.
This is a generalization of the previous model and includes possible non-linear reactions and time-dependencies.
\end{abstract}

\section{Introduction}
The goal of the project is to compare different strategies for modelling the behaviour of juvenile zebrafish.
These models should be able to predict and simulate the individual motion of one animal reacting to its environment (e.g. another fish and a wall).

A use-case for this project is steering a virtual fish in a virtual reality environment for animals.
It can be used to perform experiments that investigate causal relationships in animal behaviour.

The movement of zebrafish can be described by discrete models that assume that the fish moves in a piece-wise linear fashion.
After choosing a heading direction, the fish kicks off and moves in an approximately straight line.
We model the heading change for each kick.

We start with an simple model and refine it twice.
Each modification drops assumptions about the behaviour and thus creates a more flexible, data-driven model.
The models developed in the earlier steps serve as baselines for the more complicated ones.
We can thus see whether the assumptions are correct and how important each component of the model is.

We make the following contributions:
\begin{itemize}
\item We adapt a wall model to zebrafish.
\item We develop an adaptive receptive field approach for efficient spatial binning.
\item We evaluate the effect of time-dependet receptive fields.
\item We present a recurrent neural network that can handle bi-modal predictions and captures non-linear effects.

\end{itemize}
\section{Pre-Processing}

\subsection{Discrete Motion}
The movement of zebrafish can be described by discrete models that assume that the fish moves in a piece-wise linear fashion.
After choosing a heading direction, the fish kicks off and moves in an approximately straight line.
We are interested in modelling the trajectory, which consist of heading change and length of heading, for each kick.
To fully describe the motion we also present simple models for duration and kick speed.

We need to segment the fish motion into kicks first.
The raw data consists of the positions of both fish, their orientation relative to the x-axis, and time.
We mark frames that caused problems for the tracking system, for example when the identity of both fish was confused, as invalid.
The video data has roughly 100 frames per second, the time between frames is slightly irregular.
We convert the video to a constant framerate by iterating over the frames and insert frames between them if the time between frames differs by more than 0.005 seconds from a constant framerate.
These inserted frames are marked invalid as well.

The velocity is computed by finite differences between timepoints.
We then smooth the velocity by a Savgol filter with window length of 15 frames and a polynomial order of three.
This filter uses a local polynomial approximation, it is also used to compute the derivative of the velocity.
If fish move slower than CONSTANT for CONSTANT s, they are marked as stopped.
Frames in that area are marked as invalid.
We do this because we are interested in swimming behavior.

We then use the zero crossing of this smoothed acceleration to mark instances, where the fish moves from accelerating to gliding.
One acceleration and gliding phase is then combined to form a kick.
If an acceleration or gliding phase is shorter than a treshold\todo{Name it} the phase is moved into the previous one.

% kick_columns = [ 'fish_id', 'heading_change', 'heading_change_acc', 'duration', 'gliding_duration', 'length', 'max_vel', 'end_vel']
% social_columns = ['neighbor_distance', 'neighbor_angle', 'geometric_leader', 'viewing_angle_ltf', 'viewing_angle_ftl', 'rel_orientation',
%                     'x_f0', 'y_f0', 'x_f1', 'y_f1', 'angle_f0', 'angle_f1']
% wall_columns = [ f"wall_{type}{wall}_{id}" for id, type, wall in product( ['f0', 'f1'], ['distance', 'angle'],[0,1,2,3] )]

We then extract the heading change, length, maximum velocity for each kick.
Additionally, we extract the positions and angles of both fish and the distance and angles towards the wall for both fish for the kick and timesteps before it.
We include this data for the timesteps \(0, 5, \ldots, 30 \).

\subsection{Wall forces}
We begin with the modeling of wall-forces.
We follow the modeling approach of~\autocite{calovi} and describe the influence of a wall on the heading change \(\delta \phi_w\) as
\begin{equation*}
  \delta \phi_w (r_w, \theta_w) = f(r_w)O_w(\theta_w),
\end{equation*}
where $r_w$ corresponds to the distance to the wall and $\theta$ is the relative angle of the fish towars the wall.
We split the wall influence into an exponential decaying force term \(f_w\) and an odd angular-response function \(O_w\).
%todo: multiply by a constant
\begin{align*}
  f(r_w) &= \exp\left( -{(r_w/l_w)}^2 \right), \\
  O(\theta_w) &= \left(a_1 \sin(\theta_w) + a_2 \sin(2  \theta_w)  \right)  \left(1 +  b_1  \cos(\theta_w) + b_2 \cos(2  \theta_w) \right),
\end{align*}
where $l_w, a_1, a_2, b_1 \text{ and } b_2$ are parameters.
Note that we use a different series expansion for the odd angular function \(O_w\)
\footnote{Their proposed form does not work in our case.
  The reason for this could be that they consider both a different species and a round wall.}.
Finally, we consider the closest two walls and sum over the influences
\begin{equation*}
 \delta \phi_w^{\text{total}} \left( \bm{r_w}, \bm{\theta_w} \right) = \sum_{i \in 2 \text{ closest walls}} \delta \phi_w^i (r_w^i, \theta_w^i).
\end{equation*}
The parameters are fit by minimizing the mean-squared error of heading change prediction using gradient-descent.
% todo: we do not actually use gradient descent but something different.

\subsection{Receptive Fields}
We now explain the input features of our social models.
From here on, we restrict our dataset to kicks where the wall influence is neglible.
Practically, we drop all kicks where the value of \(f_w(r_w)\) is larger than $0.1$.

Our social model use a receptive field as their input~\autocite{discreteModes}.
We construct this by rotating the coordinate system such that the fish we are considering is parallel to the x-axis.
This corresponds to a heading of zero degrees.
We then shift the coordinate system such that our fish is at the center.

Or models then try to predict the product of a vector in direction of the heading change and the kick length.
This correspond to the kick trajectory in our new coordinate system.

The position is discretized using adaptive spatial bins that are constructed by\(\cdots\).
\marginnote{This can be generalized to multiple fish by interpreting the features as number of fish per bin and mean relative angle per bin.}
This categorical feature bin is converted using one-hot-encoding to a feature vector.
We additionally use the standarized relative trajectory of the other fish multiplied with the aforementioned spatial feature.

\section{Social Models}
We are now ready to model the social behaviour of zebrafish.
Here, we only consider the situation with one other fish for reasons of simplicity.
Note that the approaches developed here can all be easily extended to larger fish groups.
We start with a simple linear model.
This model is then refined twice by adding memory and non-linear effects.

\subsection{First linear model}
We begin with a basic linear model.
The linear model for a component \(i\) of our target vector \(\hat{\bm{y}}\) can be written as
\begin{equation*}
 \left( Y^i | \bm{\beta}^{i} \right)  \sim \mathcal{N} \left( \bm{X^i} \bm{\beta^i} + \text{ bias}, \sigma^2 \bm{I_{n \times p}}  \right),
\end{equation*}
where X is the design matrix.
In the following discussion we will drop the superscript for the component and describe the computation for one vector component without loss of generality.

\subsection{Adding a time dependence}
We now add a time-dependence to our model.
To do this, we extract some time window before each kick.
For our experiments we use time steps of \(\SI{0}{\s}, \SI{0.05}{s}, \ldots, \SI{0.25}{\s}\), which goes back roughly to the beginning of the last kick.

We consider two more models here.
For the first one we simply concatenate all features \(\beta_{i, t}\) for all timesteps \(t\).
This results in a very large number of parameters (\(\text{num. timesteps } \times \text{ num. bins}\)).

Alternatively, we can use the same spatial weights for each bin (i.e\ \( \forall t_1, t_2: \beta_{i, t_1} = \beta_{i, t_2}\)).
We then introduce a parameter \(c_t\) for each timestep and then use $c_t \beta_{i,t}$ as our parameters.
For consistency, we normalize all \(c_t\) such that they sum to unity.

\subsection{Adding non-linear effects}
As a final model we present a recurrent-neural network that predicts a full distribution for \(\left( \bm{y} | \bm{\beta} \right)\).
We start by describing an encoder, which transforms the input features to a hidden state, and a decoder, which then transforms the hidden state into the output values.
The complete model is then an arbitrary combination of presented encoders and decoders.

We describe two simple encoders:
\begin{itemize}
\item The simplest (non-linear) choice is a multilayer-perceptron.
  In our case we use a 
\end{itemize}


We follow the basic idea of \textit{mixture density networks}\autocite{mdn} but use multivariate Gaussians with non-diagonal covariance as mixture components.
To do this, we write our prediction as
\begin{equation*}
p \left( \bm{y} | \bm{\beta} \right) = \sum_{i}^n \pi_i \mathcal{N} \left( \bm{\mu_i}, \bm{\Sigma_i} \right),
\end{equation*}
where \(\pi_i, \bm{\mu_i}, \bm{\Sigma_i}\) are the parameters for a mixture of Gaussians with \(k\) components and \(\mathcal{N}(\bm{\mu}, \bm{\Sigma_i})\) is a multivariate normal distribution 

These parameters are predicted for each target seperately by our neural network.
We will now derive necessary conditions that have to hold for the parameters and describe how our network output satisfies them.
Note that we can write the Cholesky decomposition
\footnote{This decomposition always exists for invertible covariance matrices as they are by definition positive-semidefinite and symmetric.}
of the covariance matrix \(\bm{\Sigma_i}\) as
\begin{equation*}
  \bm{\Sigma_i} = \bm{L_i} \bm{L_i^T} =
  \begin{bmatrix}
    a_i^2 & a_ib_i \\
    a_ib_i & b_i^2 + c_i^2
  \end{bmatrix} ,\quad
   \text{with } \bm{L} =
   \begin{bmatrix}
     a_i & 0 \\
     b_i & c_i
   \end{bmatrix}.
\end{equation*}
We then have to fulfil the following constraints:
\begin{itemize}
\item The diagonals of all \(\bm{L}_i\) need to be larger than zero.
  To do this we apply the \textit{softplus} non-linearity
  \begin{equation*}
   \operatorname{Softplus} (\bm{x}) = \log \left( 1 + \exp (\bm{x}) \right).
  \end{equation*}
  We add a small \(\varepsilon = 10^{-4}\) for numerical stability
  \footnote{This regularization reduces the condition number of \(\bm{\Sigma_i}\) by increasing the value of its minimal eigenvalue.
  Additionally it ensures that the variance has a minimal value and thus avoids degenerate distributions.}. % maybe cite this master's thesis?
  Using the fact that a matrix has an invertible Cholesky decomposition with positive diagonal elements iff.\ it is positive definite and Hermetian,
  the resulting matrix \(\bm{\Sigma_i}\) is a valid non-singular covariance matrix.
\item The mixing coefficients need to be positive and have to sum to one.
  This is achieved by using the \textit{softmax} non-linearity
  \begin{equation*}
    \operatorname{Softmax} (x_i) = \frac{\exp (x_i)}{\sum_j \exp (x_j)},
  \end{equation*}
  where \(x_i\) is one component of the output vector.
\end{itemize}
All other parameters can have arbitrary values.
We predict the Gaussian mixture parameters each by computing a linear transformation of the hidden state of the encoder followed by the appropiate non-linearity.


\section{Implementation \textit{\&} Training Details}
All neural networks were developed with PyTorch.
We use Adam with a learning rate of 0.01, exponentially decayed (by ...) with a weight decay of 0.001.
Encoders and decoders are jointly trained.

The other linear models were trained with \textbf{scikit-learn}\autocite{scikitLearn}.
We used a grid search over the regularization parameters and chose the best w.r.t.\ the average cross-validation error.

\section{Evaluation}
% TODO: The following is not true!
%\marginnote{We can compare the log-likelihood of our models because they are \textit{nested} models, i.e.\ each model is a non-regularized \textsc{RNN} predicting parameters of mixtures of Gaussians with certain restrictions on the parameters.}
We start by comparing the angle prediction from the wall model.
It is not wise to compare the mean-squared-error or similar metrics because they do not take the cyclical nature of angles into account.
For example, the angles 0 and 360 degrees are identical.
% Angle error, https://stats.stackexchange.com/questions/197639/calculating-goodness-of-fit-for-circular-data
We use the absolute difference between two vectors of angles \(\bm{\alpha}, \bm{\beta}\)
\begin{equation*}
 \operatorname{Angle-Error}(\bm{\alpha}, \bm{\beta}) = \operatorname{arctan2} \left( \sum_i \sin \left( \alpha_i - \beta_i  \right),  \sum_i \sin \left( \alpha_i - \beta_i \right)  \right),
\end{equation*}
which is zero if both angles are identical~\autocite{circularStatistics}.
Predicting the average angle (defined by \todo{what?}) leads to an error of xxx, using our model of xxx.
This error was estimated over the complete training and testing datasets.

We compare the models by the log-likelihood on the training and testing set.
As mentioned in chaper xxx, both datasets are restricted to situations with neglible wall-force.
This comparison captures the whole predictive distribution and not only the mean prediction.
In this way we take the uncertainty of the predictions into account.
Note that it is not straight-forward to use more complicated methods such as log-likelihood ratio or the Akaike information criterion because we cannot estimate the number of degrees of freedoms for penalized recurrent neural network models.

The results for the social models can be seen in table xxx.


\section{Conclusion}
We presented different models for the behaviour of zebrafish.

Even though extending the models with a temporal and non-linear component did not show significant improvements, we think that the approaches are promising.
\begin{itemize}
\item The adaptive spatial discretization presented here results in a more equal distribution of fish.
\item Because the social models do not use properties of other animals directly but rather model the mean features per bin they extend trivially to larger animal groups, without changing the statistical models or the spatial discretization.
  This extension is harder for force-based models and usually require extensive modifications.
\item The explicit modelling of probability distributions allows us to sample from the trajectory distribution conditioned on the social surroundings and is able to predict bimodal distributions.
\end{itemize}

\printbibliography
\end{document}